{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "autoreload works on IPython versions 6.0 and higher. It reloads modules after\n",
    "you save them.\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "'''\n",
    "%reload_ext autoreload\n",
    "'''\n",
    "Mark modules for autoreload explicitly.\n",
    "Note, we have to use 1 instead of explicit. For some reason the explicit parameter\n",
    "never autoreloads aimport modules.\n",
    "'''\n",
    "%autoreload 1\n",
    "\n",
    "%aimport final_project_group_13_utlis\n",
    "from final_project_group_13_utlis import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\n",
    "    r'C:\\Users\\Michael Metz\\Documents\\Education\\MEng Software\\2023-01 - 2023-04 Winter\\ENEL 645\\BreakHis\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast'\n",
    ")\n",
    "'''\n",
    "The path where the two class folders (benign, malignant) of images reside.\n",
    "\n",
    "raw strings can be used in-case you use a Windows path with `\\`.\n",
    "\n",
    "If you want any other paths in this script to be cross platform, you *must* use\n",
    "the forward slash `/` to make the paths work on Linux or Mac. But if you are\n",
    "just using a path on only a Windows machine (like this dataset_path) you can\n",
    "use `\\`.\n",
    "\n",
    "Also note you can't end a raw string with a `\\` (and don't need to in this case\n",
    "as we just need the path up to the folder)\n",
    "'''\n",
    "\n",
    "class_list = ['benign', 'malignant']\n",
    "'''\n",
    "List of expected class subfolders in the dataset folder.\n",
    "'''\n",
    "\n",
    "train_split = 0.6\n",
    "'''\n",
    "Set the train split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "validation_split = 0.1\n",
    "'''\n",
    "Set the validation split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "test_split = 1.0 - validation_split - train_split\n",
    "'''\n",
    "Set the test split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "random_seed = 154\n",
    "'''\n",
    "Used for configuring a consistent random_seed where we need randomness with\n",
    "reproducable results, like when shuffling the order of images.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "'''\n",
    "The batch_size for training.\n",
    "'''\n",
    "\n",
    "image_size = (224, 340)\n",
    "'''\n",
    "The image size of all images in the dataset.\n",
    "'''\n",
    "\n",
    "crop_size = (224, 224)\n",
    "'''\n",
    "The size to randomly crop all images to during preprocessing (including train, validation, test).\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Enable memory growth for the first GPU detected instead of assigning all\n",
    "of the device memory during runtime initialization\n",
    "\"\"\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stratify Split the data into development and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not copying files as C:\\Users\\Michael Metz\\Documents\\Education\\MEng Software\\2023-01 - 2023-04 Winter\\ENEL 645\\BreakHis\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast\\split_data already esists\n",
      "Development benign set length: 1736\n",
      "Test benign set length: 744\n",
      "Development benign image count: 1736\n",
      "Test benign image count: 744\n",
      "Training benign set length: 1488\n",
      "Validation benign set length: 248\n",
      "Training benign image count: 1488\n",
      "Validation benign image count: 248\n",
      "\n",
      "Development malignant set length: 3800\n",
      "Test malignant set length: 1629\n",
      "Development malignant image count: 3800\n",
      "Test malignant image count: 1629\n",
      "Training malignant set length: 3257\n",
      "Validation malignant set length: 543\n",
      "Training malignant image count: 3257\n",
      "Validation malignant image count: 543\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\training',\n",
       " 'C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\validation',\n",
       " 'C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\test']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset_path, validation_dataset_path, test_dataset_path \\\n",
    "    = stratified_train_val_test_split_into_folders(\n",
    "        dataset_path,\n",
    "        move=False,\n",
    "        train_split=train_split,\n",
    "        validation_split=validation_split,\n",
    "        test_split=test_split,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "display([training_dataset_path, validation_dataset_path, test_dataset_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4745 files belonging to 2 classes.\n",
      "Found 791 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
      "Found 2373 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds, val_ds = preprocess_train_val(\n",
    "    training_dataset_path,\n",
    "    validation_dataset_path,\n",
    "    image_size=image_size,\n",
    "    crop_size=crop_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = preprocess_test(\n",
    "    test_dataset_path,\n",
    "    image_size=image_size,\n",
    "    crop_size=crop_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "display([train_ds, val_ds, test_ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image_Input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " VGG_Preprocess (Lambda)        (None, 224, 224, 3)  0           ['Image_Input[0][0]']            \n",
      "                                                                                                  \n",
      " vgg16_features (Functional)    (None, 14, 14, 512)  7635264     ['VGG_Preprocess[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 14, 14, 64)   32832       ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 14, 14, 128)  589952      ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 14, 14, 32)   409632      ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 512)  0          ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 14, 14, 736)  0           ['conv2d_15[0][0]',              \n",
      "                                                                  'conv2d_16[0][0]',              \n",
      "                                                                  'conv2d_17[0][0]',              \n",
      "                                                                  'max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " BN (BatchNormalization)        (None, 14, 14, 736)  2944        ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 144256)       0           ['BN[0][0]']                     \n",
      "                                                                                                  \n",
      " Dropout (Dropout)              (None, 144256)       0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " Predictions (Dense)            (None, 2)            288514      ['Dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,959,138\n",
      "Trainable params: 1,322,402\n",
      "Non-trainable params: 7,636,736\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "38/38 [==============================] - 34s 689ms/step - loss: 2.0641 - accuracy: 0.8194 - f1_score: 0.7897 - mae: 0.1818 - val_loss: 4.2659 - val_accuracy: 0.8748 - val_f1_score: 0.8421 - val_mae: 0.1256 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = vgginnet_builder()\n",
    "best_model_file = 'vgginnet_best_model.h5'\n",
    "\n",
    "train_validate(\n",
    "    model, train_ds, val_ds, best_model_file=best_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 8s 321ms/step - loss: 1.5145 - accuracy: 0.9254 - f1_score: 0.9150 - mae: 0.0745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.5145031213760376,\n",
       " 'accuracy': 0.9254108667373657,\n",
       " 'f1_score_benign_malignant': array([0.8852884 , 0.94473934], dtype=float32),\n",
       " 'mae': 0.07449477910995483}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = test(best_model_file, test_ds)\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENEL645-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
