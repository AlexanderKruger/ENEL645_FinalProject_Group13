{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_project_group_13 import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\n",
    "    r'C:\\Users\\Michael Metz\\Documents\\Education\\MEng Software\\2023-01 - 2023-04 Winter\\ENEL 645\\BreakHis\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast'\n",
    ")\n",
    "'''\n",
    "The path where the two class folders (benign, malignant) of images reside.\n",
    "\n",
    "raw strings can be used in-case you use a Windows path with `\\`.\n",
    "\n",
    "If you want any other paths in this script to be cross platform, you *must* use\n",
    "the forward slash `/` to make the paths work on Linux or Mac. But if you are\n",
    "just using a path on only a Windows machine (like this DATASET_PATH) you can\n",
    "use `\\`.\n",
    "\n",
    "Also note you can't end a raw string with a `\\` (and don't need to in this case\n",
    "as we just need the path up to the folder)\n",
    "'''\n",
    "\n",
    "class_list = ['benign', 'malignant']\n",
    "'''\n",
    "List of expected class subfolders in the dataset folder.\n",
    "'''\n",
    "\n",
    "train_split = 0.6\n",
    "'''\n",
    "Set the train split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "validation_split = 0.1\n",
    "'''\n",
    "Set the validation split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "test_split = 1.0 - validation_split - train_split\n",
    "'''\n",
    "Set the test split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "random_seed = 154\n",
    "'''\n",
    "Used for configuring a consistent RANDOM_SEED where we need randomness with\n",
    "reproducable results, like when shuffling the order of images.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "'''\n",
    "The batch_size for training.\n",
    "'''\n",
    "\n",
    "image_size = (224, 340)\n",
    "'''\n",
    "The image size of all images in the dataset.\n",
    "'''\n",
    "\n",
    "crop_size = (224, 224)\n",
    "'''\n",
    "The size to randomly crop all images to during preprocessing (including train, validation, test).\n",
    "'''\n",
    "\n",
    "model_name = \"group_13_best_model.h5\"\n",
    "'''\n",
    "The name of the file to save the best model to (in .h5 format).\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "Enable memory growth for the first GPU detected instead of assigning all\n",
    "of the device memory during runtime initialization\n",
    "\"\"\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stratify Split the data into development and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not copying files as C:\\Users\\Michael Metz\\Documents\\Education\\MEng Software\\2023-01 - 2023-04 Winter\\ENEL 645\\BreakHis\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast\\split_data already esists\n",
      "Development benign set length: 1736\n",
      "Test benign set length: 744\n",
      "Development benign image count: 1736\n",
      "Test benign image count: 744\n",
      "Training benign set length: 1488\n",
      "Validation benign set length: 248\n",
      "Training benign image count: 1488\n",
      "Validation benign image count: 248\n",
      "\n",
      "Development malignant set length: 3800\n",
      "Test malignant set length: 1629\n",
      "Development malignant image count: 3800\n",
      "Test malignant image count: 1629\n",
      "Training malignant set length: 3257\n",
      "Validation malignant set length: 543\n",
      "Training malignant image count: 3257\n",
      "Validation malignant image count: 543\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\training',\n",
       " 'C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\validation',\n",
       " 'C:\\\\Users\\\\Michael Metz\\\\Documents\\\\Education\\\\MEng Software\\\\2023-01 - 2023-04 Winter\\\\ENEL 645\\\\BreakHis\\\\BreaKHis_v1\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\split_data\\\\test']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_dataset_path, validation_dataset_path, test_dataset_path \\\n",
    "    = stratified_train_val_test_split_into_folders(\n",
    "        dataset_path,\n",
    "        move=False,\n",
    "        train_split=train_split,\n",
    "        validation_split=validation_split,\n",
    "        test_split=test_split,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "display([training_dataset_path, validation_dataset_path, test_dataset_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4745 files belonging to 2 classes.\n",
      "Found 791 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "Found 2373 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds, val_ds = preprocess_train_val(\n",
    "    training_dataset_path,\n",
    "    validation_dataset_path,\n",
    "    image_size=image_size,\n",
    "    crop_size=crop_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = preprocess_test(\n",
    "    test_dataset_path,\n",
    "    image_size=image_size,\n",
    "    crop_size=crop_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "display([train_ds, val_ds, test_ds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgginnet_builder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image_Input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " VGG_Preprocess (Lambda)        (None, 224, 224, 3)  0           ['Image_Input[0][0]']            \n",
      "                                                                                                  \n",
      " vgg16_features (Functional)    (None, 14, 14, 512)  7635264     ['VGG_Preprocess[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 64)   32832       ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 14, 14, 128)  589952      ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 14, 14, 32)   409632      ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 14, 14, 512)  0           ['vgg16_features[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 14, 736)  0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]',               \n",
      "                                                                  'conv2d_2[0][0]',               \n",
      "                                                                  'max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " BN (BatchNormalization)        (None, 14, 14, 736)  2944        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 144256)       0           ['BN[0][0]']                     \n",
      "                                                                                                  \n",
      " Dropout (Dropout)              (None, 144256)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " Predictions (Dense)            (None, 2)            288514      ['Dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,959,138\n",
      "Trainable params: 1,322,402\n",
      "Non-trainable params: 7,636,736\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 35s 416ms/step - loss: 2.0652 - accuracy: 0.8255 - f1_score: 0.7976 - mae: 0.1726 - val_loss: 3.5954 - val_accuracy: 0.8546 - val_f1_score: 0.8390 - val_mae: 0.1475 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 20s 452ms/step - loss: 1.4657 - accuracy: 0.8704 - f1_score: 0.8492 - mae: 0.1298 - val_loss: 4.1963 - val_accuracy: 0.8698 - val_f1_score: 0.8307 - val_mae: 0.1320 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 21s 468ms/step - loss: 1.2618 - accuracy: 0.8864 - f1_score: 0.8682 - mae: 0.1134 - val_loss: 4.2480 - val_accuracy: 0.8344 - val_f1_score: 0.7752 - val_mae: 0.1635 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 20s 447ms/step - loss: 1.0561 - accuracy: 0.9052 - f1_score: 0.8896 - mae: 0.0961 - val_loss: 2.7091 - val_accuracy: 0.8786 - val_f1_score: 0.8521 - val_mae: 0.1207 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 20s 449ms/step - loss: 1.1526 - accuracy: 0.9045 - f1_score: 0.8894 - mae: 0.0956 - val_loss: 2.8502 - val_accuracy: 0.8673 - val_f1_score: 0.8393 - val_mae: 0.1307 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 20s 456ms/step - loss: 1.3296 - accuracy: 0.9050 - f1_score: 0.8891 - mae: 0.0959 - val_loss: 2.3529 - val_accuracy: 0.8951 - val_f1_score: 0.8733 - val_mae: 0.1064 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 20s 446ms/step - loss: 1.1375 - accuracy: 0.9218 - f1_score: 0.9091 - mae: 0.0791 - val_loss: 2.2552 - val_accuracy: 0.9039 - val_f1_score: 0.8884 - val_mae: 0.0961 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 20s 426ms/step - loss: 0.9745 - accuracy: 0.9250 - f1_score: 0.9125 - mae: 0.0755 - val_loss: 3.6449 - val_accuracy: 0.8116 - val_f1_score: 0.8009 - val_mae: 0.1885 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 21s 458ms/step - loss: 1.2960 - accuracy: 0.9146 - f1_score: 0.9004 - mae: 0.0855 - val_loss: 3.0719 - val_accuracy: 0.8521 - val_f1_score: 0.8404 - val_mae: 0.1481 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 21s 480ms/step - loss: 1.0018 - accuracy: 0.9317 - f1_score: 0.9209 - mae: 0.0688 - val_loss: 2.5316 - val_accuracy: 0.8913 - val_f1_score: 0.8793 - val_mae: 0.1111 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 20s 442ms/step - loss: 0.8963 - accuracy: 0.9355 - f1_score: 0.9254 - mae: 0.0648 - val_loss: 2.3174 - val_accuracy: 0.9052 - val_f1_score: 0.8907 - val_mae: 0.0966 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 20s 446ms/step - loss: 0.9490 - accuracy: 0.9412 - f1_score: 0.9318 - mae: 0.0601 - val_loss: 2.6137 - val_accuracy: 0.8938 - val_f1_score: 0.8758 - val_mae: 0.1027 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 20s 458ms/step - loss: 0.6952 - accuracy: 0.9431 - f1_score: 0.9341 - mae: 0.0570 - val_loss: 2.3015 - val_accuracy: 0.9178 - val_f1_score: 0.9025 - val_mae: 0.0830 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 21s 464ms/step - loss: 0.7335 - accuracy: 0.9479 - f1_score: 0.9394 - mae: 0.0523 - val_loss: 2.4909 - val_accuracy: 0.8951 - val_f1_score: 0.8828 - val_mae: 0.1041 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 21s 470ms/step - loss: 0.7139 - accuracy: 0.9482 - f1_score: 0.9397 - mae: 0.0520 - val_loss: 3.3575 - val_accuracy: 0.8635 - val_f1_score: 0.8524 - val_mae: 0.1385 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 21s 459ms/step - loss: 0.5228 - accuracy: 0.9595 - f1_score: 0.9530 - mae: 0.0406 - val_loss: 2.5419 - val_accuracy: 0.9052 - val_f1_score: 0.8927 - val_mae: 0.0954 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 20s 457ms/step - loss: 0.4425 - accuracy: 0.9621 - f1_score: 0.9560 - mae: 0.0386 - val_loss: 2.3655 - val_accuracy: 0.9052 - val_f1_score: 0.8933 - val_mae: 0.0936 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 20s 443ms/step - loss: 0.4670 - accuracy: 0.9648 - f1_score: 0.9592 - mae: 0.0354 - val_loss: 2.5576 - val_accuracy: 0.8989 - val_f1_score: 0.8865 - val_mae: 0.1002 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 19s 437ms/step - loss: 0.4405 - accuracy: 0.9652 - f1_score: 0.9596 - mae: 0.0348 - val_loss: 2.2850 - val_accuracy: 0.9140 - val_f1_score: 0.9026 - val_mae: 0.0864 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 21s 477ms/step - loss: 0.4921 - accuracy: 0.9606 - f1_score: 0.9544 - mae: 0.0388 - val_loss: 2.2289 - val_accuracy: 0.9191 - val_f1_score: 0.9050 - val_mae: 0.0814 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 21s 484ms/step - loss: 0.3294 - accuracy: 0.9722 - f1_score: 0.9677 - mae: 0.0281 - val_loss: 1.9549 - val_accuracy: 0.9191 - val_f1_score: 0.9060 - val_mae: 0.0779 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 21s 456ms/step - loss: 0.2340 - accuracy: 0.9781 - f1_score: 0.9745 - mae: 0.0225 - val_loss: 1.9674 - val_accuracy: 0.9254 - val_f1_score: 0.9140 - val_mae: 0.0758 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 20s 428ms/step - loss: 0.2874 - accuracy: 0.9745 - f1_score: 0.9704 - mae: 0.0259 - val_loss: 2.1345 - val_accuracy: 0.9128 - val_f1_score: 0.9015 - val_mae: 0.0885 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 20s 446ms/step - loss: 0.2306 - accuracy: 0.9749 - f1_score: 0.9709 - mae: 0.0256 - val_loss: 1.9119 - val_accuracy: 0.9216 - val_f1_score: 0.9107 - val_mae: 0.0775 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 20s 449ms/step - loss: 0.2085 - accuracy: 0.9777 - f1_score: 0.9741 - mae: 0.0228 - val_loss: 1.8692 - val_accuracy: 0.9305 - val_f1_score: 0.9197 - val_mae: 0.0710 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 21s 472ms/step - loss: 0.2006 - accuracy: 0.9791 - f1_score: 0.9759 - mae: 0.0216 - val_loss: 1.8939 - val_accuracy: 0.9229 - val_f1_score: 0.9115 - val_mae: 0.0762 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 21s 461ms/step - loss: 0.1840 - accuracy: 0.9745 - f1_score: 0.9703 - mae: 0.0247 - val_loss: 1.9847 - val_accuracy: 0.9241 - val_f1_score: 0.9130 - val_mae: 0.0767 - lr: 1.2500e-04\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 20s 467ms/step - loss: 0.2109 - accuracy: 0.9796 - f1_score: 0.9762 - mae: 0.0208 - val_loss: 1.9297 - val_accuracy: 0.9279 - val_f1_score: 0.9169 - val_mae: 0.0745 - lr: 1.2500e-04\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 21s 461ms/step - loss: 0.2258 - accuracy: 0.9804 - f1_score: 0.9772 - mae: 0.0195 - val_loss: 1.8780 - val_accuracy: 0.9305 - val_f1_score: 0.9198 - val_mae: 0.0715 - lr: 6.2500e-05\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 20s 446ms/step - loss: 0.1510 - accuracy: 0.9840 - f1_score: 0.9814 - mae: 0.0163 - val_loss: 1.8324 - val_accuracy: 0.9241 - val_f1_score: 0.9123 - val_mae: 0.0736 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 21s 454ms/step - loss: 0.1991 - accuracy: 0.9787 - f1_score: 0.9753 - mae: 0.0211 - val_loss: 1.7857 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0728 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 21s 484ms/step - loss: 0.2393 - accuracy: 0.9781 - f1_score: 0.9746 - mae: 0.0216 - val_loss: 1.8304 - val_accuracy: 0.9267 - val_f1_score: 0.9152 - val_mae: 0.0746 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 20s 456ms/step - loss: 0.1252 - accuracy: 0.9848 - f1_score: 0.9824 - mae: 0.0152 - val_loss: 1.8396 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0739 - lr: 3.1250e-05\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 20s 454ms/step - loss: 0.1274 - accuracy: 0.9842 - f1_score: 0.9816 - mae: 0.0161 - val_loss: 1.8142 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0750 - lr: 3.1250e-05\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 20s 449ms/step - loss: 0.1735 - accuracy: 0.9825 - f1_score: 0.9797 - mae: 0.0181 - val_loss: 1.8176 - val_accuracy: 0.9254 - val_f1_score: 0.9133 - val_mae: 0.0754 - lr: 3.1250e-05\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 19s 431ms/step - loss: 0.1171 - accuracy: 0.9842 - f1_score: 0.9817 - mae: 0.0156 - val_loss: 1.8169 - val_accuracy: 0.9267 - val_f1_score: 0.9148 - val_mae: 0.0743 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 20s 448ms/step - loss: 0.1387 - accuracy: 0.9827 - f1_score: 0.9799 - mae: 0.0176 - val_loss: 1.8043 - val_accuracy: 0.9292 - val_f1_score: 0.9178 - val_mae: 0.0722 - lr: 1.5625e-05\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 21s 468ms/step - loss: 0.1903 - accuracy: 0.9812 - f1_score: 0.9783 - mae: 0.0185 - val_loss: 1.7873 - val_accuracy: 0.9292 - val_f1_score: 0.9179 - val_mae: 0.0726 - lr: 1.5625e-05\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 21s 462ms/step - loss: 0.1539 - accuracy: 0.9823 - f1_score: 0.9794 - mae: 0.0174 - val_loss: 1.7840 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0734 - lr: 1.5625e-05\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 20s 462ms/step - loss: 0.1529 - accuracy: 0.9838 - f1_score: 0.9812 - mae: 0.0164 - val_loss: 1.7625 - val_accuracy: 0.9191 - val_f1_score: 0.9070 - val_mae: 0.0774 - lr: 1.5625e-05\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 20s 442ms/step - loss: 0.1657 - accuracy: 0.9834 - f1_score: 0.9807 - mae: 0.0167 - val_loss: 1.7644 - val_accuracy: 0.9216 - val_f1_score: 0.9097 - val_mae: 0.0773 - lr: 7.8125e-06\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 21s 460ms/step - loss: 0.1397 - accuracy: 0.9834 - f1_score: 0.9807 - mae: 0.0171 - val_loss: 1.7732 - val_accuracy: 0.9229 - val_f1_score: 0.9111 - val_mae: 0.0765 - lr: 7.8125e-06\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 20s 445ms/step - loss: 0.1472 - accuracy: 0.9827 - f1_score: 0.9799 - mae: 0.0178 - val_loss: 1.7751 - val_accuracy: 0.9229 - val_f1_score: 0.9111 - val_mae: 0.0771 - lr: 7.8125e-06\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 21s 469ms/step - loss: 0.1409 - accuracy: 0.9836 - f1_score: 0.9809 - mae: 0.0163 - val_loss: 1.7753 - val_accuracy: 0.9216 - val_f1_score: 0.9099 - val_mae: 0.0779 - lr: 7.8125e-06\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 21s 464ms/step - loss: 0.1526 - accuracy: 0.9823 - f1_score: 0.9795 - mae: 0.0180 - val_loss: 1.7779 - val_accuracy: 0.9216 - val_f1_score: 0.9099 - val_mae: 0.0780 - lr: 3.9063e-06\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 19s 433ms/step - loss: 0.1575 - accuracy: 0.9802 - f1_score: 0.9771 - mae: 0.0198 - val_loss: 1.7791 - val_accuracy: 0.9267 - val_f1_score: 0.9154 - val_mae: 0.0760 - lr: 3.9063e-06\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 21s 456ms/step - loss: 0.1646 - accuracy: 0.9812 - f1_score: 0.9782 - mae: 0.0191 - val_loss: 1.7768 - val_accuracy: 0.9254 - val_f1_score: 0.9138 - val_mae: 0.0753 - lr: 3.9063e-06\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 20s 452ms/step - loss: 0.1362 - accuracy: 0.9812 - f1_score: 0.9782 - mae: 0.0190 - val_loss: 1.7762 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0745 - lr: 3.9063e-06\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 19s 436ms/step - loss: 0.0983 - accuracy: 0.9893 - f1_score: 0.9875 - mae: 0.0117 - val_loss: 1.7697 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0743 - lr: 1.9531e-06\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 20s 425ms/step - loss: 0.1585 - accuracy: 0.9834 - f1_score: 0.9807 - mae: 0.0169 - val_loss: 1.7718 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0752 - lr: 1.9531e-06\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 21s 471ms/step - loss: 0.1410 - accuracy: 0.9838 - f1_score: 0.9811 - mae: 0.0166 - val_loss: 1.7718 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0730 - lr: 1.9531e-06\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 20s 459ms/step - loss: 0.1338 - accuracy: 0.9819 - f1_score: 0.9789 - mae: 0.0181 - val_loss: 1.7714 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0730 - lr: 1.9531e-06\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 20s 458ms/step - loss: 0.1249 - accuracy: 0.9863 - f1_score: 0.9841 - mae: 0.0141 - val_loss: 1.7714 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0727 - lr: 9.7656e-07\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 21s 450ms/step - loss: 0.1840 - accuracy: 0.9817 - f1_score: 0.9787 - mae: 0.0187 - val_loss: 1.7728 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0728 - lr: 9.7656e-07\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 20s 449ms/step - loss: 0.1528 - accuracy: 0.9825 - f1_score: 0.9797 - mae: 0.0174 - val_loss: 1.7759 - val_accuracy: 0.9279 - val_f1_score: 0.9166 - val_mae: 0.0727 - lr: 9.7656e-07\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 20s 446ms/step - loss: 0.1737 - accuracy: 0.9812 - f1_score: 0.9782 - mae: 0.0193 - val_loss: 1.7786 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0729 - lr: 9.7656e-07\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 21s 462ms/step - loss: 0.1555 - accuracy: 0.9825 - f1_score: 0.9796 - mae: 0.0183 - val_loss: 1.7759 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0731 - lr: 4.8828e-07\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 19s 429ms/step - loss: 0.1426 - accuracy: 0.9831 - f1_score: 0.9804 - mae: 0.0170 - val_loss: 1.7754 - val_accuracy: 0.9267 - val_f1_score: 0.9148 - val_mae: 0.0727 - lr: 4.8828e-07\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 19s 434ms/step - loss: 0.1669 - accuracy: 0.9819 - f1_score: 0.9789 - mae: 0.0188 - val_loss: 1.7765 - val_accuracy: 0.9267 - val_f1_score: 0.9148 - val_mae: 0.0729 - lr: 4.8828e-07\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 20s 463ms/step - loss: 0.1281 - accuracy: 0.9850 - f1_score: 0.9826 - mae: 0.0153 - val_loss: 1.7720 - val_accuracy: 0.9267 - val_f1_score: 0.9150 - val_mae: 0.0729 - lr: 4.8828e-07\n"
     ]
    }
   ],
   "source": [
    "train_validate(model, train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 11s 494ms/step - loss: 1.5145 - accuracy: 0.9254 - f1_score: 0.9150 - mae: 0.0745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5145363807678223,\n",
       " 0.9254108667373657,\n",
       " array([0.8852884 , 0.94473934], dtype=float32),\n",
       " 0.07449565827846527]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = test(model_name, test_ds)\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENEL645-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
