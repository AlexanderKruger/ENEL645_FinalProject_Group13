{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "'''\n",
    "Change back to tensorflow.keras to turn on lazy loading of imports and to\n",
    "match the exact keras version that tensorflow uses as of tensorflow 2.10\n",
    "'''\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from keras.applications.resnet import preprocess_input as resnet_preprocess\n",
    "from keras.models import Model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Lambda, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Concatenate, BatchNormalization\n",
    "from keras.layers import Dropout, Dense, Flatten\n",
    "from keras.layers import Dropout, GlobalAveragePooling2D, Dense, Flatten, Activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe name of the file to save the best model to (in .h5 format).\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "'''\n",
    "---\n",
    "Configuration params\n",
    "---\n",
    "\n",
    "Changing these after importing doesn't affect function defaults, but do affect\n",
    "functions that use these configurations directly.\n",
    "'''\n",
    "\n",
    "dataset_path = Path(\n",
    "    'BreaKHis_v1/BreaKHis_v1/histology_slides/breast'\n",
    ")\n",
    "'''\n",
    "The path where the two class folders (benign, malignant) of images reside.\n",
    "\n",
    "raw strings can be used in-case you use a Windows path with `\\`.\n",
    "\n",
    "If you want any other paths in this script to be cross platform, you *must* use\n",
    "the forward slash `/` to make the paths work on Linux or Mac. But if you are\n",
    "just using a path on only a Windows machine (like this DATASET_PATH) you can\n",
    "use `\\`.\n",
    "\n",
    "Also note you can't end a raw string with a `\\` (and don't need to in this case\n",
    "as we just need the path up to the folder)\n",
    "'''\n",
    "\n",
    "class_list = ['benign', 'malignant']\n",
    "'''\n",
    "List of expected class subfolders in the dataset folder.\n",
    "'''\n",
    "\n",
    "train_split = 0.6\n",
    "'''\n",
    "Set the train split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "validation_split = 0.1\n",
    "'''\n",
    "Set the validation split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "test_split = 1.0 - validation_split - train_split\n",
    "'''\n",
    "Set the test split. Train, validation, test split must add up to approximately 1.0.\n",
    "'''\n",
    "\n",
    "random_seed = 154\n",
    "'''\n",
    "Used for configuring a consistent RANDOM_SEED where we need randomness with\n",
    "reproducable results, like when shuffling the order of images.\n",
    "'''\n",
    "\n",
    "batch_size = 128\n",
    "'''\n",
    "The batch_size for training.\n",
    "'''\n",
    "\n",
    "image_size = (224, 340)\n",
    "'''\n",
    "The image size of all images in the dataset.\n",
    "'''\n",
    "\n",
    "crop_size = (224, 224)\n",
    "'''\n",
    "The size to randomly crop all images to during preprocessing (including train, validation, test).\n",
    "'''\n",
    "\n",
    "model_name = \"group_13_best_model.h5\"\n",
    "'''\n",
    "The name of the file to save the best model to (in .h5 format).\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stratified_train_val_test_split_into_folders(\n",
    "        dataset_path,\n",
    "        *,\n",
    "        class_list=class_list,\n",
    "        split_data_path=None,\n",
    "        move=False,\n",
    "        train_split=train_split,\n",
    "        validation_split=validation_split,\n",
    "        test_split=test_split,\n",
    "        random_seed=random_seed):\n",
    "    \"\"\"\n",
    "    Loops through the `class_list` and splits the data set into train, test,\n",
    "    and validation datasets. The images will be in `split_data_path`/`\n",
    "\n",
    "    Args:\n",
    "        dataset_path (Path, optional): The folder that contains the class folders with pngs in the class folders or any folder below. Defaults to DATASET_PATH.\n",
    "        class_list (list, optional): List of expected class subfolders. Defaults to class_list.\n",
    "        split_data_path (Path, optional): Where to output the split data. Defaults to None (meaning dataset_path/'split_data').\n",
    "        move (bool, optional): Move files from `dataset_path` if True, else copy the files. Defaults to False.\n",
    "        train_split (float, optional): Amount to split into training. Defaults to train_split.\n",
    "        validation_split (float, optional): Amount to split into validation. Defaults to validation_split.\n",
    "        test_split (float, optional): Amount to split into test. Defaults to test_split.\n",
    "        random_seed (int, optional): random seed to use for shuffling. Defaults to random_seed.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: All splits must add up to approximately 1.0, if they don't this is raised.\n",
    "\n",
    "    Returns:\n",
    "        list(str): A list of strings, one each for train, validation, test path.\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN_FOLDER_NAME = 'training'\n",
    "    VALIDATION_FOLDER_NAME = 'validation'\n",
    "    TEST_FOLDER_NAME = 'test'\n",
    "\n",
    "    if split_data_path is None:\n",
    "        split_data_path = dataset_path / 'split_data'\n",
    "\n",
    "    split_total = train_split + validation_split + test_split\n",
    "    EXPECTED_SPLIT_TOTAL = 1.0\n",
    "    if not math.isclose(split_total, EXPECTED_SPLIT_TOTAL):\n",
    "        raise ValueError(\n",
    "            'train_split + validation_split + test_split ({}) is not approximately = {}'.format(split_total, EXPECTED_SPLIT_TOTAL))\n",
    "\n",
    "    copy_move_str = 'Copying'\n",
    "    if move:\n",
    "        copy_move_str = 'Moving'\n",
    "\n",
    "    development_split = train_split + validation_split\n",
    "\n",
    "    destination_paths = []\n",
    "\n",
    "    allow_move_or_copy = True\n",
    "\n",
    "    if split_data_path.exists():\n",
    "        print(\n",
    "            f\"Not {copy_move_str.lower()} files as {split_data_path} already esists\")\n",
    "        allow_move_or_copy = False\n",
    "\n",
    "    split_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_index, class_ in enumerate(class_list):\n",
    "        class_images = glob(\n",
    "            str(dataset_path / class_ / '**/*.png'), recursive=True)\n",
    "\n",
    "        # Shuffles the list in place.\n",
    "        random.Random(random_seed).shuffle(class_images)\n",
    "\n",
    "        development_length = int(development_split * len(class_images))\n",
    "\n",
    "        print(f'Development {class_} set length: {development_length}')\n",
    "        print(\n",
    "            f'Test {class_} set length: {len(class_images) - development_length}')\n",
    "\n",
    "        development_class_image_paths = class_images[:development_length]\n",
    "        test_class_image_paths = class_images[development_length:]\n",
    "\n",
    "        print(\n",
    "            f'Development {class_} image count: {len(development_class_image_paths)}')\n",
    "        print(f'Test {class_} image count: {len(test_class_image_paths)}')\n",
    "\n",
    "        '''\n",
    "        / does float division in python3 and we expect these numbers to be float\n",
    "        anyways.\n",
    "\n",
    "        TRAIN_SPLIT is relative to DEVELOPMENT_SPLIT images because we are working\n",
    "        with an images subset, and the numbers are absolute to the total dataset.\n",
    "        '''\n",
    "        training_length = int(train_split / development_split *\n",
    "                              len(development_class_image_paths))\n",
    "\n",
    "        print(f'Training {class_} set length: {training_length}')\n",
    "        print(\n",
    "            f'Validation {class_} set length: {len(development_class_image_paths) - training_length}')\n",
    "\n",
    "        training_class_image_paths = development_class_image_paths[:training_length]\n",
    "        validation_class_image_paths = development_class_image_paths[training_length:]\n",
    "        print(\n",
    "            f'Training {class_} image count: {len(training_class_image_paths)}')\n",
    "        print(\n",
    "            f'Validation {class_} image count: {len(validation_class_image_paths)}')\n",
    "\n",
    "        split_folder_name_split_image_class_paths_dict = {\n",
    "            TRAIN_FOLDER_NAME: training_class_image_paths,\n",
    "            VALIDATION_FOLDER_NAME: validation_class_image_paths,\n",
    "            TEST_FOLDER_NAME: test_class_image_paths\n",
    "        }\n",
    "\n",
    "        print()\n",
    "\n",
    "        for split_folder_name, split_class_image_paths in split_folder_name_split_image_class_paths_dict.items():\n",
    "            split_path = split_data_path / split_folder_name\n",
    "            destination_path: Path = split_path / class_\n",
    "\n",
    "            '''\n",
    "            Only append destination paths and make the split folders on the\n",
    "            first class_ iteration. We don't want duplicate folders.\n",
    "            '''\n",
    "            if class_index == 0:\n",
    "                destination_paths.append(str(split_path))\n",
    "                split_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "            '''\n",
    "            Make the class folder in each split folder\n",
    "            '''\n",
    "            destination_path.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "            if allow_move_or_copy:\n",
    "                print(\n",
    "                    f'{copy_move_str} {split_folder_name} files from {dataset_path} to {destination_path}')\n",
    "                for split_class_image_path in split_class_image_paths:\n",
    "                    if move == True:\n",
    "                        shutil.move(split_class_image_path,\n",
    "                                    str(destination_path))\n",
    "                    else:\n",
    "                        shutil.copy(split_class_image_path,\n",
    "                                    str(destination_path))\n",
    "\n",
    "        '''\n",
    "        If not the last iteration and allow_move_or_copy...\n",
    "        '''\n",
    "        if class_index != len(class_list) and allow_move_or_copy:\n",
    "            print()\n",
    "\n",
    "    return destination_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_val(\n",
    "        training_dataset_path,\n",
    "        validation_dataset_path,\n",
    "        *,\n",
    "        image_size=image_size,\n",
    "        crop_size=crop_size,\n",
    "        batch_size=batch_size):\n",
    "    \"\"\" \n",
    "    This function will take parameters for the datas file path along with the image, crop, and batch size. It will then perform the training\n",
    "    set's cropping and data augmentation and return the dataset once it is transformed.\n",
    "    \"\"\"\n",
    "\n",
    "    crop_layer = tf.keras.layers.CenterCrop(*crop_size)\n",
    "    augmentation_layer = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip(),\n",
    "            tf.keras.layers.RandomRotation((-0.2, 0.2), seed=random_seed),\n",
    "            tf.keras.layers.RandomContrast(0.1, seed=random_seed),\n",
    "            tf.keras.layers.RandomHeight(0.2, seed=random_seed),\n",
    "            tf.keras.layers.RandomWidth(0.2, seed=random_seed),\n",
    "            tf.keras.layers.Resizing(224, 340, crop_to_aspect_ratio=True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        training_dataset_path,\n",
    "        shuffle=True,\n",
    "        label_mode='categorical',\n",
    "        seed=random_seed,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size)\n",
    "\n",
    "    validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        validation_dataset_path,\n",
    "        shuffle=False,\n",
    "        label_mode='categorical',\n",
    "        seed=random_seed,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size)\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda image, label: (\n",
    "            augmentation_layer(image, training=True),\n",
    "            label),\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda image, label: (\n",
    "            crop_layer(image, training=True),\n",
    "            label),\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    validation_ds = validation_ds.map(\n",
    "        lambda image, label: (\n",
    "            crop_layer(image, training=True),\n",
    "            label),\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return train_ds.prefetch(buffer_size=AUTOTUNE), validation_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(\n",
    "        path,\n",
    "        *,\n",
    "        image_size=image_size,\n",
    "        crop_size=crop_size,\n",
    "        batch_size=batch_size):\n",
    "\n",
    "    crop_layer = tf.keras.layers.CenterCrop(*crop_size)\n",
    "\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        path,\n",
    "        label_mode='categorical',\n",
    "        seed=random_seed,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size)\n",
    "\n",
    "    test_ds = test_ds.map(\n",
    "        lambda image, label: (\n",
    "            crop_layer(image, training=True),\n",
    "            label),\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return test_ds.prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET\n",
    "def resnet50_builder():# Defining the model\n",
    "    base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        weights='imagenet',  \n",
    "        input_shape=(224,224,3),\n",
    "        include_top=False) \n",
    "    base_model.trainable = False\n",
    "\n",
    "    x1 = base_model(base_model.input, training = False)\n",
    "    x2 = tf.keras.layers.Flatten()(x1)\n",
    "\n",
    "\n",
    "    out = tf.keras.layers.Dense(2, activation = 'softmax')(x2)\n",
    "    model = tf.keras.Model(inputs = base_model.input, outputs =out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19\n",
    "\n",
    "def vgginnet_builder():\n",
    "    base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    layer_name = 'block4_pool'\n",
    "    feature_ex_model = Model(inputs=base_model.input, \n",
    "                             outputs=base_model.get_layer(layer_name).output, \n",
    "                             name='vgg16_features')\n",
    "    feature_ex_model.trainable = False\n",
    "\n",
    "    p1_layer = Lambda(vgg_preprocess, name='VGG_Preprocess')\n",
    "    image_input = Input((224, 224, 3), name='Image_Input')\n",
    "    p1_tensor = p1_layer(image_input)\n",
    "\n",
    "    out =feature_ex_model(p1_tensor)\n",
    "    feature_ex_model = Model(inputs=image_input, outputs=out)\n",
    "\n",
    "    def naive_inception_module(layer_in, f1, f2, f3):\n",
    "        # 1x1 conv\n",
    "        conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "        # 3x3 conv\n",
    "        conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
    "        # 5x5 conv\n",
    "        conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
    "        # 3x3 max pooling\n",
    "        pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "        # concatenate filters, assumes filters/channels last\n",
    "        layer_out = Concatenate()([conv1, conv3, conv5, pool])\n",
    "        return layer_out\n",
    "\n",
    "    out = naive_inception_module(feature_ex_model.output, 64, 128, 32)\n",
    "    num_classes = 2\n",
    "\n",
    "    bn1 = BatchNormalization(name='BN')(out)\n",
    "    f = Flatten()(bn1)\n",
    "    dropout = Dropout(0.4, name='Dropout')(f)\n",
    "    desne = Dense(num_classes, activation='softmax', name='Predictions')(dropout)\n",
    "\n",
    "    model = Model(inputs=feature_ex_model.input, outputs=desne)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnetnaive_builder():\n",
    "    base_model = tf.keras.applications.resnet50.ResNet50(\n",
    "        weights='imagenet',  \n",
    "        input_shape=(224,224,3),\n",
    "        include_top=False) \n",
    "    \n",
    "\n",
    "    layer_name = 'conv5_block3_out'\n",
    "    feature_ex_model = Model(inputs=base_model.input, \n",
    "                             outputs=base_model.get_layer(layer_name).output, \n",
    "                             name='resnet50_features')\n",
    "    feature_ex_model.trainable = False\n",
    "\n",
    "    p1_layer = Lambda(resnet_preprocess, name='Resnet_Preprocess')\n",
    "    image_input = Input((224, 224, 3), name='Image_Input')\n",
    "    p1_tensor = p1_layer(image_input)\n",
    "\n",
    "    out =feature_ex_model(p1_tensor)\n",
    "    feature_ex_model = Model(inputs=image_input, outputs=out)\n",
    "\n",
    "    def naive_inception_module(layer_in, f1, f2, f3):\n",
    "        # 1x1 conv\n",
    "        conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "        # 3x3 conv\n",
    "        conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
    "        # 5x5 conv\n",
    "        conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
    "        # 3x3 max pooling\n",
    "        pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "        # concatenate filters, assumes filters/channels last\n",
    "        layer_out = Concatenate()([conv1, conv3, conv5, pool])\n",
    "        return layer_out\n",
    "\n",
    "    out = naive_inception_module(feature_ex_model.output, 64, 128, 32)\n",
    "    num_classes = 2\n",
    "\n",
    "    bn1 = BatchNormalization(name='BN')(out)\n",
    "    f = Flatten()(bn1)\n",
    "    dropout = Dropout(0.4, name='Dropout')(f)\n",
    "    desne = Dense(num_classes, activation='softmax', name='Predictions')(dropout)\n",
    "\n",
    "    model = Model(inputs=feature_ex_model.input, outputs=desne)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(model: Model, train_ds, val_ds, epochs=5, learning_rate=1e-4):\n",
    "\n",
    "    #\n",
    "    # Define your callbacks (save best model, early stopping, learning rate scheduler)\n",
    "    #\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20)\n",
    "\n",
    "    monitor = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_name,\n",
    "        monitor='val_loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='min')\n",
    "\n",
    "    # Learning rate schedule\n",
    "    # Reduce learning rate every 4 epochs.\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch % 4 == 0 and epoch != 0:\n",
    "            lr = lr/2\n",
    "        return lr\n",
    "\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "        scheduler,\n",
    "        verbose=0)\n",
    "\n",
    "    # Show model summary before training.\n",
    "    print(model.summary())\n",
    "\n",
    "    #\n",
    "    # Configure and train the model\n",
    "    #\n",
    "\n",
    "    # Define optimizer, loss function, and metrics.\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stop, monitor, lr_schedule],\n",
    "        validation_data=(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: Model, test_ds: tf.data.Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_ds: Expects test_ds to be preprocessed for pre-trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.load_weights(model_name)\n",
    "    metrics = model.evaluate(test_ds)\n",
    "\n",
    "    Ypred = model.predict(test_ds).argmax(axis=1)\n",
    "    label_batch_list = []\n",
    "    for _, label_batch in test_ds:\n",
    "        label_batch_list.append(label_batch)\n",
    "    Y_test_t = tf.concat(label_batch_list, axis=0)\n",
    "    Y_test = Y_test_t.numpy()\n",
    "\n",
    "    wrong_indexes = np.where(Ypred != Y_test)[0]\n",
    "\n",
    "    return metrics, wrong_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development benign set length: 1736\n",
      "Test benign set length: 744\n",
      "Development benign image count: 1736\n",
      "Test benign image count: 744\n",
      "Training benign set length: 1488\n",
      "Validation benign set length: 248\n",
      "Training benign image count: 1488\n",
      "Validation benign image count: 248\n",
      "\n",
      "Copying training files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\training\\benign\n",
      "Copying validation files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\validation\\benign\n",
      "Copying test files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\test\\benign\n",
      "\n",
      "Development malignant set length: 3800\n",
      "Test malignant set length: 1629\n",
      "Development malignant image count: 3800\n",
      "Test malignant image count: 1629\n",
      "Training malignant set length: 3257\n",
      "Validation malignant set length: 543\n",
      "Training malignant image count: 3257\n",
      "Validation malignant image count: 543\n",
      "\n",
      "Copying training files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\training\\malignant\n",
      "Copying validation files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\validation\\malignant\n",
      "Copying test files from D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast to D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast\\split_data\\test\\malignant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path(\n",
    "    r'D:\\BreakHisDataset\\BreaKHis_v1\\histology_slides\\breast'\n",
    ")\n",
    "training_dataset_path,validation_dataset_path, test_dataset_path = stratified_train_val_test_split_into_folders(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4745 files belonging to 2 classes.\n",
      "Found 791 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "Found 2373 files belonging to 2 classes.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image_Input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Resnet_Preprocess (Lambda)     (None, 224, 224, 3)  0           ['Image_Input[0][0]']            \n",
      "                                                                                                  \n",
      " resnet50_features (Functional)  (None, 7, 7, 2048)  23587712    ['Resnet_Preprocess[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 7, 7, 64)     131136      ['resnet50_features[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 128)    2359424     ['resnet50_features[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 7, 7, 32)     1638432     ['resnet50_features[0][0]']      \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 7, 7, 2048)   0           ['resnet50_features[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 7, 7, 2272)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]',               \n",
      "                                                                  'conv2d_2[0][0]',               \n",
      "                                                                  'max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " BN (BatchNormalization)        (None, 7, 7, 2272)   9088        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 111328)       0           ['BN[0][0]']                     \n",
      "                                                                                                  \n",
      " Dropout (Dropout)              (None, 111328)       0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " Predictions (Dense)            (None, 2)            222658      ['Dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,948,450\n",
      "Trainable params: 4,356,194\n",
      "Non-trainable params: 23,592,256\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "149/149 [==============================] - 40s 173ms/step - loss: 0.5691 - accuracy: 0.8270 - val_loss: 0.4099 - val_accuracy: 0.8887 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "149/149 [==============================] - 26s 167ms/step - loss: 0.3486 - accuracy: 0.8959 - val_loss: 0.3822 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "149/149 [==============================] - 26s 164ms/step - loss: 0.3283 - accuracy: 0.9031 - val_loss: 0.4333 - val_accuracy: 0.8837 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "149/149 [==============================] - 26s 164ms/step - loss: 0.2904 - accuracy: 0.9159 - val_loss: 0.4115 - val_accuracy: 0.8951 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "149/149 [==============================] - 26s 168ms/step - loss: 0.2227 - accuracy: 0.9340 - val_loss: 0.3771 - val_accuracy: 0.9090 - lr: 5.0000e-05\n",
      "Epoch 6/30\n",
      "149/149 [==============================] - 26s 171ms/step - loss: 0.1874 - accuracy: 0.9429 - val_loss: 0.2806 - val_accuracy: 0.9279 - lr: 5.0000e-05\n",
      "Epoch 7/30\n",
      "149/149 [==============================] - 25s 159ms/step - loss: 0.2083 - accuracy: 0.9423 - val_loss: 0.3230 - val_accuracy: 0.9229 - lr: 5.0000e-05\n",
      "Epoch 8/30\n",
      "149/149 [==============================] - 25s 163ms/step - loss: 0.1578 - accuracy: 0.9501 - val_loss: 0.3165 - val_accuracy: 0.9128 - lr: 5.0000e-05\n",
      "Epoch 9/30\n",
      "149/149 [==============================] - 26s 166ms/step - loss: 0.1365 - accuracy: 0.9562 - val_loss: 0.2929 - val_accuracy: 0.9128 - lr: 2.5000e-05\n",
      "Epoch 10/30\n",
      "149/149 [==============================] - 26s 166ms/step - loss: 0.1207 - accuracy: 0.9614 - val_loss: 0.2635 - val_accuracy: 0.9254 - lr: 2.5000e-05\n",
      "Epoch 11/30\n",
      "149/149 [==============================] - 25s 163ms/step - loss: 0.1176 - accuracy: 0.9587 - val_loss: 0.2709 - val_accuracy: 0.9241 - lr: 2.5000e-05\n",
      "Epoch 12/30\n",
      "149/149 [==============================] - 26s 168ms/step - loss: 0.1119 - accuracy: 0.9629 - val_loss: 0.2363 - val_accuracy: 0.9305 - lr: 2.5000e-05\n",
      "Epoch 13/30\n",
      "149/149 [==============================] - 26s 166ms/step - loss: 0.1144 - accuracy: 0.9631 - val_loss: 0.2337 - val_accuracy: 0.9418 - lr: 1.2500e-05\n",
      "Epoch 14/30\n",
      "149/149 [==============================] - 26s 165ms/step - loss: 0.0955 - accuracy: 0.9669 - val_loss: 0.2436 - val_accuracy: 0.9368 - lr: 1.2500e-05\n",
      "Epoch 15/30\n",
      "149/149 [==============================] - 27s 174ms/step - loss: 0.1120 - accuracy: 0.9633 - val_loss: 0.2305 - val_accuracy: 0.9355 - lr: 1.2500e-05\n",
      "Epoch 16/30\n",
      "149/149 [==============================] - 27s 176ms/step - loss: 0.0850 - accuracy: 0.9686 - val_loss: 0.2358 - val_accuracy: 0.9330 - lr: 1.2500e-05\n",
      "Epoch 17/30\n",
      "149/149 [==============================] - 26s 169ms/step - loss: 0.0892 - accuracy: 0.9705 - val_loss: 0.2251 - val_accuracy: 0.9355 - lr: 6.2500e-06\n",
      "Epoch 18/30\n",
      "149/149 [==============================] - 25s 163ms/step - loss: 0.0860 - accuracy: 0.9707 - val_loss: 0.2258 - val_accuracy: 0.9368 - lr: 6.2500e-06\n",
      "Epoch 19/30\n",
      "149/149 [==============================] - 26s 166ms/step - loss: 0.0763 - accuracy: 0.9728 - val_loss: 0.2271 - val_accuracy: 0.9431 - lr: 6.2500e-06\n",
      "Epoch 20/30\n",
      "149/149 [==============================] - 26s 163ms/step - loss: 0.0729 - accuracy: 0.9751 - val_loss: 0.2291 - val_accuracy: 0.9406 - lr: 6.2500e-06\n",
      "Epoch 21/30\n",
      "149/149 [==============================] - 26s 170ms/step - loss: 0.0716 - accuracy: 0.9762 - val_loss: 0.2246 - val_accuracy: 0.9418 - lr: 3.1250e-06\n",
      "Epoch 22/30\n",
      "149/149 [==============================] - 27s 173ms/step - loss: 0.0711 - accuracy: 0.9726 - val_loss: 0.2231 - val_accuracy: 0.9393 - lr: 3.1250e-06\n",
      "Epoch 23/30\n",
      "149/149 [==============================] - 26s 167ms/step - loss: 0.0654 - accuracy: 0.9777 - val_loss: 0.2195 - val_accuracy: 0.9406 - lr: 3.1250e-06\n",
      "Epoch 24/30\n",
      "149/149 [==============================] - 27s 172ms/step - loss: 0.0709 - accuracy: 0.9749 - val_loss: 0.2185 - val_accuracy: 0.9418 - lr: 3.1250e-06\n",
      "Epoch 25/30\n",
      "149/149 [==============================] - 26s 166ms/step - loss: 0.0694 - accuracy: 0.9777 - val_loss: 0.2188 - val_accuracy: 0.9381 - lr: 1.5625e-06\n",
      "Epoch 26/30\n",
      "149/149 [==============================] - 26s 168ms/step - loss: 0.0770 - accuracy: 0.9753 - val_loss: 0.2184 - val_accuracy: 0.9393 - lr: 1.5625e-06\n",
      "Epoch 27/30\n",
      "149/149 [==============================] - 26s 167ms/step - loss: 0.0776 - accuracy: 0.9747 - val_loss: 0.2154 - val_accuracy: 0.9393 - lr: 1.5625e-06\n",
      "Epoch 28/30\n",
      "149/149 [==============================] - 26s 167ms/step - loss: 0.0862 - accuracy: 0.9703 - val_loss: 0.2161 - val_accuracy: 0.9381 - lr: 1.5625e-06\n",
      "Epoch 29/30\n",
      "149/149 [==============================] - 26s 172ms/step - loss: 0.0694 - accuracy: 0.9747 - val_loss: 0.2147 - val_accuracy: 0.9418 - lr: 7.8125e-07\n",
      "Epoch 30/30\n",
      "149/149 [==============================] - 28s 180ms/step - loss: 0.0739 - accuracy: 0.9732 - val_loss: 0.2156 - val_accuracy: 0.9406 - lr: 7.8125e-07\n",
      "75/75 [==============================] - 11s 114ms/step - loss: 0.2560 - accuracy: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2560290992259979, 0.9325748085975647]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds = preprocess_train_val(training_dataset_path,validation_dataset_path, batch_size=32)\n",
    "test_ds = preprocess_test(test_dataset_path, batch_size=32)\n",
    "model = resnetnaive_builder()\n",
    "train_validate(model, train_ds, val_ds, epochs=30)\n",
    "\n",
    "model = tf.keras.models.load_model(model_name)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENEL645",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
