{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "from final_project_group_13_utlis import *\n",
            "\n",
            "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
            "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset_path = Path(\n",
            "    r'C:\\Users\\feagm\\Desktop\\ENEL_645\\Project\\archive\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast'\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Not copying files as C:\\Users\\feagm\\Desktop\\ENEL_645\\Project\\archive\\BreaKHis_v1\\BreaKHis_v1\\histology_slides\\breast\\split_data already esists\n",
                  "Development benign set length: 1736\n",
                  "Test benign set length: 744\n",
                  "Development benign image count: 1736\n",
                  "Test benign image count: 744\n",
                  "Training benign set length: 1488\n",
                  "Validation benign set length: 248\n",
                  "Training benign image count: 1488\n",
                  "Validation benign image count: 248\n",
                  "\n",
                  "Development malignant set length: 3800\n",
                  "Test malignant set length: 1629\n",
                  "Development malignant image count: 3800\n",
                  "Test malignant image count: 1629\n",
                  "Training malignant set length: 3257\n",
                  "Validation malignant set length: 543\n",
                  "Training malignant image count: 3257\n",
                  "Validation malignant image count: 543\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "training_dataset_path, validation_dataset_path, test_dataset_path \\\n",
            "    = stratified_train_val_test_split_into_folders(\n",
            "        dataset_path,\n",
            "        move=False,\n",
            "        train_split=train_split,\n",
            "        validation_split=validation_split,\n",
            "        test_split=test_split,\n",
            "        random_seed=random_seed\n",
            "    )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Found 4745 files belonging to 2 classes.\n",
                  "Found 791 files belonging to 2 classes.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformIntV2 cause there is no registered converter for this op.\n"
               ]
            }
         ],
         "source": [
            "train_ds,val_ds = preprocess_train_val(training_dataset_path=training_dataset_path, validation_dataset_path=validation_dataset_path, batch_size=32)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Found 2373 files belonging to 2 classes.\n"
               ]
            }
         ],
         "source": [
            "test_ds = preprocess_test(path=test_dataset_path, batch_size=32)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "model= resnetnaive_builder(layer_name='conv2_block3_out')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"model_1\"\n",
                  "__________________________________________________________________________________________________\n",
                  " Layer (type)                   Output Shape         Param #     Connected to                     \n",
                  "==================================================================================================\n",
                  " Image_Input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
                  "                                )]                                                                \n",
                  "                                                                                                  \n",
                  " Resnet_Preprocess (Lambda)     (None, 224, 224, 3)  0           ['Image_Input[0][0]']            \n",
                  "                                                                                                  \n",
                  " resnet50_features (Functional)  (None, 56, 56, 256)  229760     ['Resnet_Preprocess[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d (Conv2D)                (None, 56, 56, 64)   16448       ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d_1 (Conv2D)              (None, 56, 56, 128)  295040      ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d_2 (Conv2D)              (None, 56, 56, 32)   204832      ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " max_pooling2d (MaxPooling2D)   (None, 56, 56, 256)  0           ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " concatenate (Concatenate)      (None, 56, 56, 480)  0           ['conv2d[0][0]',                 \n",
                  "                                                                  'conv2d_1[0][0]',               \n",
                  "                                                                  'conv2d_2[0][0]',               \n",
                  "                                                                  'max_pooling2d[0][0]']          \n",
                  "                                                                                                  \n",
                  " BN (BatchNormalization)        (None, 56, 56, 480)  1920        ['concatenate[0][0]']            \n",
                  "                                                                                                  \n",
                  " flatten (Flatten)              (None, 1505280)      0           ['BN[0][0]']                     \n",
                  "                                                                                                  \n",
                  " Dropout (Dropout)              (None, 1505280)      0           ['flatten[0][0]']                \n",
                  "                                                                                                  \n",
                  " Predictions (Dense)            (None, 2)            3010562     ['Dropout[0][0]']                \n",
                  "                                                                                                  \n",
                  "==================================================================================================\n",
                  "Total params: 3,758,562\n",
                  "Trainable params: 3,527,842\n",
                  "Non-trainable params: 230,720\n",
                  "__________________________________________________________________________________________________\n",
                  "None\n",
                  "Epoch 1/5\n",
                  "149/149 [==============================] - 112s 644ms/step - loss: 23.2389 - accuracy: 0.7947 - f1_score: 0.7612 - mae: 0.2059 - val_loss: 25.7995 - val_accuracy: 0.7914 - val_f1_score: 0.7297 - val_mae: 0.2098 - lr: 0.0010\n",
                  "Epoch 2/5\n",
                  "149/149 [==============================] - 78s 455ms/step - loss: 21.7151 - accuracy: 0.8078 - f1_score: 0.7774 - mae: 0.1924 - val_loss: 23.1145 - val_accuracy: 0.8154 - val_f1_score: 0.7689 - val_mae: 0.1842 - lr: 0.0010\n",
                  "Epoch 3/5\n",
                  "149/149 [==============================] - 74s 423ms/step - loss: 23.2616 - accuracy: 0.8133 - f1_score: 0.7832 - mae: 0.1868 - val_loss: 34.8593 - val_accuracy: 0.8192 - val_f1_score: 0.7546 - val_mae: 0.1809 - lr: 0.0010\n",
                  "Epoch 4/5\n",
                  "149/149 [==============================] - 64s 366ms/step - loss: 21.5911 - accuracy: 0.8240 - f1_score: 0.7951 - mae: 0.1765 - val_loss: 30.3863 - val_accuracy: 0.8104 - val_f1_score: 0.7645 - val_mae: 0.1906 - lr: 0.0010\n",
                  "Epoch 5/5\n",
                  "149/149 [==============================] - 62s 359ms/step - loss: 20.5645 - accuracy: 0.8325 - f1_score: 0.8055 - mae: 0.1680 - val_loss: 17.7464 - val_accuracy: 0.8622 - val_f1_score: 0.8394 - val_mae: 0.1392 - lr: 0.0010\n"
               ]
            }
         ],
         "source": [
            "train_validate(model, train_ds, val_ds, epochs=5, learning_rate=1e-3)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "USE CELLS BELLOW after dataset is prepared with test and train folders"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "IMAGE_SIZE = (224, 340)\n",
            "CROP_SIZE = (224, 224)\n",
            "BATCH_SIZE = 32\n",
            "\n",
            "AUTOTUNE = tf.data.AUTOTUNE\n",
            "\n",
            "# Felipe\n",
            "#image_train_val = \"C:\\\\Users\\\\feagm\\Desktop\\\\ENEL_645\\\\Project\\\\archive\\\\\"\n",
            "#image_test = \"C:\\\\Users\\\\feagm\\Desktop\\\\ENEL_645\\\\Project\\\\archive\\\\test\"\n",
            "\n",
            "# Alex\n",
            "#image_train_val = \"D:\\\\BreakHis_Dataset\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\all_model_data\\\\train\"\n",
            "#image_test = \"D:\\\\BreakHis_Dataset\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\all_model_data\\\\test\"\n",
            "\n",
            "\n",
            "def data_prep_train_val(path,\n",
            "                        image_size,\n",
            "                        crop_size,\n",
            "                        batch_size):\n",
            "    \"\"\" \n",
            "    This function will take parameters for the datas file path along with the image, crop, and batch size. It will then perform the training\n",
            "    set's cropping and data augmentation and return the dataset once it is transformed.\n",
            "    \"\"\"\n",
            "\n",
            "    crop_layer = tf.keras.layers.CenterCrop(*crop_size)\n",
            "    augmentation_layer = tf.keras.Sequential([tf.keras.layers.RandomFlip(), tf.keras.layers.RandomRotation((-0.2, 0.2),seed=34), \n",
            "                                              tf.keras.layers.RandomContrast(0.3, seed=34),\n",
            "                                              tf.keras.layers.RandomBrightness(0.3, seed=34),\n",
            "                                              tf.keras.layers.RandomTranslation([-0.2,0.2],[-0.2,0.2],seed=34),\n",
            "                                              tf.keras.layers.RandomZoom(0.25,seed=34),\n",
            "                                              tf.keras.layers.RandomHeight(0.2, seed=34),tf.keras.layers.RandomWidth(0.2, seed=34),\n",
            "                                              tf.keras.layers.Resizing(224,340,crop_to_aspect_ratio=True)])\n",
            "\n",
            "    train_ds, validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
            "        path, shuffle=True, label_mode='categorical', validation_split=0.10 / 0.70, subset=\"both\", seed=154,\n",
            "        batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n",
            "\n",
            "    \n",
            "    train_ds = train_ds.map(lambda image, label: (augmentation_layer(\n",
            "        image, training=True), label), num_parallel_calls=AUTOTUNE)\n",
            "    train_ds = train_ds.map(lambda image, label: (crop_layer(\n",
            "        image, training=True), label), num_parallel_calls=AUTOTUNE)\n",
            "\n",
            "    validation_ds = validation_ds.map(lambda image, label: (\n",
            "        crop_layer(image, training=True), label), num_parallel_calls=AUTOTUNE)\n",
            "\n",
            "    return train_ds.prefetch(buffer_size=AUTOTUNE), validation_ds.prefetch(buffer_size=AUTOTUNE)\n",
            "\n",
            "\n",
            "def data_prep_test(path,\n",
            "                   image_size,\n",
            "                   crop_size,\n",
            "                   batch_size\n",
            "                   ):\n",
            "\n",
            "    crop_layer = tf.keras.layers.CenterCrop(*crop_size)\n",
            "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
            "        path, label_mode='categorical', seed=154,\n",
            "        batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n",
            "\n",
            "    test_ds = test_ds.map(lambda image, label: (crop_layer(\n",
            "        image, training=True), label), num_parallel_calls=AUTOTUNE)\n",
            "    return test_ds.prefetch(buffer_size=AUTOTUNE)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Found 5931 files belonging to 2 classes.\n",
                  "Using 5084 files for training.\n",
                  "Using 847 files for validation.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
                  "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
               ]
            }
         ],
         "source": [
            "train, val= data_prep_train_val(\"D:\\\\BreakHis_Dataset\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\all_model_data\\\\train\",IMAGE_SIZE, CROP_SIZE, BATCH_SIZE)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Found 1978 files belonging to 2 classes.\n"
               ]
            }
         ],
         "source": [
            "test_ds = data_prep_test(\"D:\\\\BreakHis_Dataset\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\all_model_data\\\\test\",IMAGE_SIZE, CROP_SIZE, BATCH_SIZE)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Basic resnet50 model - Pretrained Imagenet"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Initial Training Model\n",
                  "Model: \"model_1\"\n",
                  "_________________________________________________________________\n",
                  " Layer (type)                Output Shape              Param #   \n",
                  "=================================================================\n",
                  " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
                  "                                                                 \n",
                  " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
                  "                                                                 \n",
                  " flatten_1 (Flatten)         (None, 100352)            0         \n",
                  "                                                                 \n",
                  " dense_1 (Dense)             (None, 2)                 200706    \n",
                  "                                                                 \n",
                  "=================================================================\n",
                  "Total params: 23,788,418\n",
                  "Trainable params: 200,706\n",
                  "Non-trainable params: 23,587,712\n",
                  "_________________________________________________________________\n",
                  "None\n"
               ]
            }
         ],
         "source": [
            "def resnet50_builder():# Defining the model\n",
            "    base_model = tf.keras.applications.resnet50.ResNet50(\n",
            "        weights='imagenet',  \n",
            "        input_shape=(224,224,3),\n",
            "        include_top=False) \n",
            "    base_model.trainable = False\n",
            "\n",
            "    x1 = base_model(base_model.input, training = False)\n",
            "    x2 = tf.keras.layers.Flatten()(x1)\n",
            "\n",
            "\n",
            "    out = tf.keras.layers.Dense(2, activation = 'softmax')(x2)\n",
            "    model = tf.keras.Model(inputs = base_model.input, outputs =out)\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "VGGINNET Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tensorflow.keras.applications.vgg16 import VGG16\n",
            "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
            "from tensorflow.keras.models import Model, Sequential\n",
            "from tensorflow.keras.layers import Lambda, Input\n",
            "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, BatchNormalization\n",
            "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Dense, Flatten, Activation\n",
            "\n",
            "\n",
            "def vgginnet_builder():\n",
            "    base_model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
            "\n",
            "    layer_name = 'block4_pool'\n",
            "    feature_ex_model = Model(inputs=base_model.input, \n",
            "                             outputs=base_model.get_layer(layer_name).output, \n",
            "                             name='vgg16_features')\n",
            "    feature_ex_model.trainable = False\n",
            "\n",
            "    p1_layer = Lambda(vgg_preprocess, name='VGG_Preprocess')\n",
            "    image_input = Input((224, 224, 3), name='Image_Input')\n",
            "    p1_tensor = p1_layer(image_input)\n",
            "\n",
            "    out =feature_ex_model(p1_tensor)\n",
            "    feature_ex_model = Model(inputs=image_input, outputs=out)\n",
            "\n",
            "    def naive_inception_module(layer_in, f1, f2, f3):\n",
            "        # 1x1 conv\n",
            "        conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
            "        # 3x3 conv\n",
            "        conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
            "        # 5x5 conv\n",
            "        conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
            "        # 3x3 max pooling\n",
            "        pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
            "        # concatenate filters, assumes filters/channels last\n",
            "        layer_out = Concatenate()([conv1, conv3, conv5, pool])\n",
            "        return layer_out\n",
            "\n",
            "    out = naive_inception_module(feature_ex_model.output, 64, 128, 32)\n",
            "    num_classes = 2\n",
            "\n",
            "    bn1 = BatchNormalization(name='BN')(out)\n",
            "    f = Flatten()(bn1)\n",
            "    dropout = Dropout(0.4, name='Dropout')(f)\n",
            "    desne = Dense(num_classes, activation='softmax', name='Predictions')(dropout)\n",
            "\n",
            "    model = Model(inputs=feature_ex_model.input, outputs=desne)\n",
            "    return model\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "ResnetNaiveModel Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tensorflow.keras.applications.vgg16 import VGG16\n",
            "from keras.applications.resnet import preprocess_input\n",
            "from tensorflow.keras.models import Model, Sequential\n",
            "from tensorflow.keras.layers import Lambda, Input\n",
            "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, BatchNormalization\n",
            "from tensorflow.keras.layers import Dropout, GlobalAveragePooling2D, Dense, Flatten, Activation\n",
            "\n",
            "\n",
            "def resnetnaive_builder():\n",
            "    base_model = tf.keras.applications.resnet50.ResNet50(\n",
            "        weights='imagenet',  \n",
            "        input_shape=(224,224,3),\n",
            "        include_top=False) \n",
            "    \n",
            "\n",
            "    layer_name = 'conv5_block3_out'\n",
            "    feature_ex_model = Model(inputs=base_model.input, \n",
            "                             outputs=base_model.get_layer(layer_name).output, \n",
            "                             name='resnet50_features')\n",
            "    feature_ex_model.trainable = False\n",
            "\n",
            "    p1_layer = Lambda(preprocess_input, name='Resnet_Preprocess')\n",
            "    image_input = Input((224, 224, 3), name='Image_Input')\n",
            "    p1_tensor = p1_layer(image_input)\n",
            "\n",
            "    out =feature_ex_model(p1_tensor)\n",
            "    feature_ex_model = Model(inputs=image_input, outputs=out)\n",
            "\n",
            "    def naive_inception_module(layer_in, f1, f2, f3):\n",
            "        # 1x1 conv\n",
            "        conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
            "        # 3x3 conv\n",
            "        conv3 = Conv2D(f2, (3,3), padding='same', activation='relu')(layer_in)\n",
            "        # 5x5 conv\n",
            "        conv5 = Conv2D(f3, (5,5), padding='same', activation='relu')(layer_in)\n",
            "        # 3x3 max pooling\n",
            "        pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
            "        # concatenate filters, assumes filters/channels last\n",
            "        layer_out = Concatenate()([conv1, conv3, conv5, pool])\n",
            "        return layer_out\n",
            "\n",
            "    out = naive_inception_module(feature_ex_model.output, 64, 128, 32)\n",
            "    num_classes = 2\n",
            "\n",
            "    bn1 = BatchNormalization(name='BN')(out)\n",
            "    f = Flatten()(bn1)\n",
            "    dropout = Dropout(0.4, name='Dropout')(f)\n",
            "    desne = Dense(num_classes, activation='softmax', name='Predictions')(dropout)\n",
            "\n",
            "    model = Model(inputs=feature_ex_model.input, outputs=desne)\n",
            "    return model"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Calling Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = resnetnaive_builder()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "from keras.models import Model, Sequential\n",
            "#best_model_path=\"C:\\\\Users\\\\feagm\\\\Desktop\\\\ENEL_645\\\\Project\\\\ENEL645_FinalProject_Group13\"\n",
            "best_model_path=\"D:\\\\BreakHis_Dataset\\\\BreaKHis_v1\\\\histology_slides\\\\breast\\\\all_model_data\"\n",
            "MODEL_NAME = \"group_13_best_model.h5\"\n",
            "def train_validate(model: Model, train_ds, val_ds, epochs=40, learning_rate=1e-4):\n",
            "\n",
            "    #\n",
            "    # Define your callbacks (save best model, early stopping, learning rate scheduler)\n",
            "    #\n",
            "\n",
            "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
            "        monitor='val_loss', patience=20)\n",
            "\n",
            "    monitor = tf.keras.callbacks.ModelCheckpoint(\n",
            "        MODEL_NAME, monitor='val_loss',\n",
            "        verbose=0, save_best_only=True,\n",
            "        save_weights_only=False,\n",
            "        mode='min')\n",
            "\n",
            "    # Learning rate schedule\n",
            "    # Reduce learning rate every 4 epochs.\n",
            "    def scheduler(epoch, lr):\n",
            "        if epoch % 4 == 0 and epoch != 0:\n",
            "            lr = lr/2\n",
            "        return lr\n",
            "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
            "        scheduler, verbose=0)\n",
            "\n",
            "    # Show model summary before training.\n",
            "    print(model.summary())\n",
            "\n",
            "    #\n",
            "    # Configure and train the model\n",
            "    #\n",
            "\n",
            "    # Define optimizer, loss function, and metrics.\n",
            "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
            "                  loss='categorical_crossentropy',\n",
            "                  metrics=['accuracy'])\n",
            "\n",
            "    model.fit(train_ds, epochs=epochs,\n",
            "              verbose=1, callbacks=[early_stop, monitor, lr_schedule], validation_data=(val_ds))\n",
            "\n",
            "\n",
            "def test(model: Model, test_ds: tf.data.Dataset):\n",
            "    \"\"\"\n",
            "    Args:\n",
            "        test_ds: Expects test_ds to be preprocessed for pre-trained model.\n",
            "    \"\"\"\n",
            "\n",
            "    model.load_weights(MODEL_NAME)\n",
            "    metrics = model.evaluate(test_ds)\n",
            "\n",
            "    Ypred = model.predict(test_ds).argmax(axis=1)\n",
            "    label_batch_list = []\n",
            "    for _, label_batch in test_ds:\n",
            "        label_batch_list.append(label_batch)\n",
            "    Y_test_t = tf.concat(label_batch_list, axis=0)\n",
            "    Y_test = Y_test_t.numpy()\n",
            "    \n",
            "    wrong_indexes = np.where(Ypred != Y_test)[0]\n",
            "\n",
            "    return metrics, wrong_indexes\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Model: \"model_3\"\n",
                  "__________________________________________________________________________________________________\n",
                  " Layer (type)                   Output Shape         Param #     Connected to                     \n",
                  "==================================================================================================\n",
                  " Image_Input (InputLayer)       [(None, 224, 224, 3  0           []                               \n",
                  "                                )]                                                                \n",
                  "                                                                                                  \n",
                  " Resnet_Preprocess (Lambda)     (None, 224, 224, 3)  0           ['Image_Input[0][0]']            \n",
                  "                                                                                                  \n",
                  " resnet50_features (Functional)  (None, 7, 7, 2048)  23587712    ['Resnet_Preprocess[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d_3 (Conv2D)              (None, 7, 7, 64)     131136      ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d_4 (Conv2D)              (None, 7, 7, 128)    2359424     ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " conv2d_5 (Conv2D)              (None, 7, 7, 32)     1638432     ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 2048)  0           ['resnet50_features[0][0]']      \n",
                  "                                                                                                  \n",
                  " concatenate_1 (Concatenate)    (None, 7, 7, 2272)   0           ['conv2d_3[0][0]',               \n",
                  "                                                                  'conv2d_4[0][0]',               \n",
                  "                                                                  'conv2d_5[0][0]',               \n",
                  "                                                                  'max_pooling2d_1[0][0]']        \n",
                  "                                                                                                  \n",
                  " BN (BatchNormalization)        (None, 7, 7, 2272)   9088        ['concatenate_1[0][0]']          \n",
                  "                                                                                                  \n",
                  " flatten_1 (Flatten)            (None, 111328)       0           ['BN[0][0]']                     \n",
                  "                                                                                                  \n",
                  " Dropout (Dropout)              (None, 111328)       0           ['flatten_1[0][0]']              \n",
                  "                                                                                                  \n",
                  " Predictions (Dense)            (None, 2)            222658      ['Dropout[0][0]']                \n",
                  "                                                                                                  \n",
                  "==================================================================================================\n",
                  "Total params: 27,948,450\n",
                  "Trainable params: 4,356,194\n",
                  "Non-trainable params: 23,592,256\n",
                  "__________________________________________________________________________________________________\n",
                  "None\n",
                  "Epoch 1/40\n",
                  "159/159 [==============================] - 45s 242ms/step - loss: 0.6816 - accuracy: 0.7872 - val_loss: 0.5887 - val_accuracy: 0.8442 - lr: 1.0000e-04\n",
                  "Epoch 2/40\n",
                  "159/159 [==============================] - 40s 236ms/step - loss: 0.5903 - accuracy: 0.8340 - val_loss: 0.5210 - val_accuracy: 0.8560 - lr: 1.0000e-04\n",
                  "Epoch 3/40\n",
                  "159/159 [==============================] - 42s 247ms/step - loss: 0.5649 - accuracy: 0.8460 - val_loss: 0.4933 - val_accuracy: 0.8784 - lr: 1.0000e-04\n",
                  "Epoch 4/40\n",
                  "159/159 [==============================] - 40s 234ms/step - loss: 0.5490 - accuracy: 0.8460 - val_loss: 0.5120 - val_accuracy: 0.8831 - lr: 1.0000e-04\n",
                  "Epoch 5/40\n",
                  "159/159 [==============================] - 35s 209ms/step - loss: 0.4830 - accuracy: 0.8672 - val_loss: 0.5157 - val_accuracy: 0.8902 - lr: 5.0000e-05\n",
                  "Epoch 6/40\n",
                  "159/159 [==============================] - 37s 217ms/step - loss: 0.4502 - accuracy: 0.8714 - val_loss: 0.4581 - val_accuracy: 0.8878 - lr: 5.0000e-05\n",
                  "Epoch 7/40\n",
                  "159/159 [==============================] - 37s 217ms/step - loss: 0.4276 - accuracy: 0.8749 - val_loss: 0.4271 - val_accuracy: 0.9032 - lr: 5.0000e-05\n",
                  "Epoch 8/40\n",
                  "159/159 [==============================] - 37s 215ms/step - loss: 0.4633 - accuracy: 0.8704 - val_loss: 0.4367 - val_accuracy: 0.9055 - lr: 5.0000e-05\n",
                  "Epoch 9/40\n",
                  "159/159 [==============================] - 37s 221ms/step - loss: 0.3896 - accuracy: 0.8865 - val_loss: 0.3877 - val_accuracy: 0.9020 - lr: 2.5000e-05\n",
                  "Epoch 10/40\n",
                  "159/159 [==============================] - 37s 221ms/step - loss: 0.3434 - accuracy: 0.8885 - val_loss: 0.3605 - val_accuracy: 0.9126 - lr: 2.5000e-05\n",
                  "Epoch 11/40\n",
                  "159/159 [==============================] - 37s 217ms/step - loss: 0.3619 - accuracy: 0.8900 - val_loss: 0.3605 - val_accuracy: 0.9008 - lr: 2.5000e-05\n",
                  "Epoch 12/40\n",
                  "159/159 [==============================] - 38s 221ms/step - loss: 0.3517 - accuracy: 0.8932 - val_loss: 0.3371 - val_accuracy: 0.9209 - lr: 2.5000e-05\n",
                  "Epoch 13/40\n",
                  "159/159 [==============================] - 38s 222ms/step - loss: 0.3264 - accuracy: 0.8965 - val_loss: 0.3507 - val_accuracy: 0.9185 - lr: 1.2500e-05\n",
                  "Epoch 14/40\n",
                  "159/159 [==============================] - 38s 225ms/step - loss: 0.3246 - accuracy: 0.8928 - val_loss: 0.3341 - val_accuracy: 0.9162 - lr: 1.2500e-05\n",
                  "Epoch 15/40\n",
                  "159/159 [==============================] - 39s 231ms/step - loss: 0.3126 - accuracy: 0.8997 - val_loss: 0.3341 - val_accuracy: 0.9209 - lr: 1.2500e-05\n",
                  "Epoch 16/40\n",
                  "159/159 [==============================] - 38s 224ms/step - loss: 0.3004 - accuracy: 0.9028 - val_loss: 0.3024 - val_accuracy: 0.9244 - lr: 1.2500e-05\n",
                  "Epoch 17/40\n",
                  "159/159 [==============================] - 38s 226ms/step - loss: 0.3015 - accuracy: 0.9020 - val_loss: 0.3147 - val_accuracy: 0.9209 - lr: 6.2500e-06\n",
                  "Epoch 18/40\n",
                  "159/159 [==============================] - 38s 226ms/step - loss: 0.2974 - accuracy: 0.9007 - val_loss: 0.3085 - val_accuracy: 0.9162 - lr: 6.2500e-06\n",
                  "Epoch 19/40\n",
                  "159/159 [==============================] - 40s 235ms/step - loss: 0.3188 - accuracy: 0.8977 - val_loss: 0.3147 - val_accuracy: 0.9138 - lr: 6.2500e-06\n",
                  "Epoch 20/40\n",
                  "159/159 [==============================] - 38s 228ms/step - loss: 0.2802 - accuracy: 0.9070 - val_loss: 0.3037 - val_accuracy: 0.9233 - lr: 6.2500e-06\n",
                  "Epoch 21/40\n",
                  "159/159 [==============================] - 39s 233ms/step - loss: 0.3023 - accuracy: 0.9046 - val_loss: 0.2981 - val_accuracy: 0.9174 - lr: 3.1250e-06\n",
                  "Epoch 22/40\n",
                  "159/159 [==============================] - 39s 228ms/step - loss: 0.2695 - accuracy: 0.9115 - val_loss: 0.2968 - val_accuracy: 0.9197 - lr: 3.1250e-06\n",
                  "Epoch 23/40\n",
                  "159/159 [==============================] - 39s 227ms/step - loss: 0.2912 - accuracy: 0.9040 - val_loss: 0.2960 - val_accuracy: 0.9209 - lr: 3.1250e-06\n",
                  "Epoch 24/40\n",
                  "159/159 [==============================] - 39s 228ms/step - loss: 0.2681 - accuracy: 0.9058 - val_loss: 0.3014 - val_accuracy: 0.9209 - lr: 3.1250e-06\n",
                  "Epoch 25/40\n",
                  "159/159 [==============================] - 38s 225ms/step - loss: 0.2914 - accuracy: 0.9062 - val_loss: 0.3005 - val_accuracy: 0.9209 - lr: 1.5625e-06\n",
                  "Epoch 26/40\n",
                  "159/159 [==============================] - 38s 226ms/step - loss: 0.2832 - accuracy: 0.9074 - val_loss: 0.2984 - val_accuracy: 0.9209 - lr: 1.5625e-06\n",
                  "Epoch 27/40\n",
                  "159/159 [==============================] - 39s 229ms/step - loss: 0.2956 - accuracy: 0.9034 - val_loss: 0.2999 - val_accuracy: 0.9209 - lr: 1.5625e-06\n",
                  "Epoch 28/40\n",
                  "159/159 [==============================] - 39s 231ms/step - loss: 0.2754 - accuracy: 0.9072 - val_loss: 0.2955 - val_accuracy: 0.9209 - lr: 1.5625e-06\n",
                  "Epoch 29/40\n",
                  "159/159 [==============================] - 40s 232ms/step - loss: 0.2814 - accuracy: 0.9072 - val_loss: 0.2968 - val_accuracy: 0.9209 - lr: 7.8125e-07\n",
                  "Epoch 30/40\n",
                  "159/159 [==============================] - 39s 229ms/step - loss: 0.2662 - accuracy: 0.9093 - val_loss: 0.2981 - val_accuracy: 0.9233 - lr: 7.8125e-07\n",
                  "Epoch 31/40\n",
                  "159/159 [==============================] - 39s 228ms/step - loss: 0.2594 - accuracy: 0.9117 - val_loss: 0.2970 - val_accuracy: 0.9221 - lr: 7.8125e-07\n",
                  "Epoch 32/40\n",
                  "159/159 [==============================] - 38s 223ms/step - loss: 0.2985 - accuracy: 0.9013 - val_loss: 0.2977 - val_accuracy: 0.9221 - lr: 7.8125e-07\n",
                  "Epoch 33/40\n",
                  "159/159 [==============================] - 40s 234ms/step - loss: 0.2797 - accuracy: 0.9081 - val_loss: 0.2970 - val_accuracy: 0.9221 - lr: 3.9062e-07\n",
                  "Epoch 34/40\n",
                  "159/159 [==============================] - 39s 230ms/step - loss: 0.2720 - accuracy: 0.9048 - val_loss: 0.2944 - val_accuracy: 0.9221 - lr: 3.9062e-07\n",
                  "Epoch 35/40\n",
                  "159/159 [==============================] - 39s 228ms/step - loss: 0.2787 - accuracy: 0.9066 - val_loss: 0.2953 - val_accuracy: 0.9233 - lr: 3.9062e-07\n",
                  "Epoch 36/40\n",
                  "159/159 [==============================] - 39s 232ms/step - loss: 0.2654 - accuracy: 0.9111 - val_loss: 0.2948 - val_accuracy: 0.9233 - lr: 3.9062e-07\n",
                  "Epoch 37/40\n",
                  "159/159 [==============================] - 38s 226ms/step - loss: 0.2783 - accuracy: 0.9062 - val_loss: 0.2947 - val_accuracy: 0.9221 - lr: 1.9531e-07\n",
                  "Epoch 38/40\n",
                  "159/159 [==============================] - 38s 227ms/step - loss: 0.2800 - accuracy: 0.9062 - val_loss: 0.2961 - val_accuracy: 0.9233 - lr: 1.9531e-07\n",
                  "Epoch 39/40\n",
                  "159/159 [==============================] - 38s 227ms/step - loss: 0.2775 - accuracy: 0.9074 - val_loss: 0.2949 - val_accuracy: 0.9233 - lr: 1.9531e-07\n",
                  "Epoch 40/40\n",
                  "159/159 [==============================] - 40s 236ms/step - loss: 0.2696 - accuracy: 0.9072 - val_loss: 0.2929 - val_accuracy: 0.9233 - lr: 1.9531e-07\n"
               ]
            }
         ],
         "source": [
            "train_validate(model, train, val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "62/62 [==============================] - 11s 138ms/step - loss: 0.2798 - accuracy: 0.9267\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "[0.27980008721351624, 0.9266936182975769]"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "model = tf.keras.models.load_model(MODEL_NAME)\n",
            "model.evaluate(test_ds)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "16/16 [==============================] - 28s 2s/step - loss: 0.3004 - accuracy: 0.8802\n"
               ]
            },
            {
               "ename": "ValueError",
               "evalue": "not enough values to unpack (expected 2, got 1)",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                  "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics,wrong_indexes \u001b[39m=\u001b[39m test(test_ds)\n",
                  "Cell \u001b[1;32mIn[14], line 49\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(test)\u001b[0m\n\u001b[0;32m     47\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(MODEL_NAME)\n\u001b[0;32m     48\u001b[0m metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test)\n\u001b[1;32m---> 49\u001b[0m X_test,Y_test \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m Ypred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\u001b[39m.\u001b[39margmax(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m wrong_indexes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(Ypred \u001b[39m!=\u001b[39m Y_test)[\u001b[39m0\u001b[39m]\n",
                  "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
               ]
            }
         ],
         "source": [
            "metrics,wrong_indexes = test(test_ds)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[0.3212464153766632, 0.8816986680030823]\n",
                  "[   0    0    1 ... 1976 1977 1977]\n"
               ]
            }
         ],
         "source": [
            "print(metrics)\n",
            "print(wrong_indexes)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "enel645",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
